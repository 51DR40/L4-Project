{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install rich accelerate gradio transformers numba datasets peft bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main Gradio interface for running and chatting with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "LGQ8BiMuXMDG",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the model to models/deepseek-ai_deepseek-coder-6.7b-instruct\n",
      "python server.py --share --model deepseek-ai_deepseek-coder-6.7b-instruct --n-gpu-layers 128 --load-in-4bit --use_double_quant\n",
      "\u001b[2;36m11:40:52-439191\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting Text generation web UI                        \n",
      "\u001b[2;36m11:40:52-443288\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m The gradio \u001b[32m\"share link\"\u001b[0m feature uses a proprietary     \n",
      "\u001b[2;36m                \u001b[0m         executable to create a reverse tunnel. Use it with     \n",
      "\u001b[2;36m                \u001b[0m         care.                                                  \n",
      "\u001b[2;36m11:40:52-444464\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m                                                        \n",
      "\u001b[2;36m                \u001b[0m         You are potentially exposing the web UI to the entire  \n",
      "\u001b[2;36m                \u001b[0m         internet without any access password.                  \n",
      "\u001b[2;36m                \u001b[0m         You can create one with the \u001b[32m\"--gradio-auth\"\u001b[0m flag like  \n",
      "\u001b[2;36m                \u001b[0m         this:                                                  \n",
      "\u001b[2;36m                \u001b[0m                                                                \n",
      "\u001b[2;36m                \u001b[0m         --gradio-auth username:password                        \n",
      "\u001b[2;36m                \u001b[0m                                                                \n",
      "\u001b[2;36m                \u001b[0m         Make sure to replace username:password with your own.  \n",
      "\u001b[2;36m11:40:52-454145\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loading \u001b[32m\"deepseek-ai_deepseek-coder-6.7b-instruct\"\u001b[0m     \n",
      "\u001b[2;36m11:40:52-458271\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[33mTRANSFORMERS_PARAMS\u001b[0m=                                   \n",
      "{   'low_cpu_mem_usage': True,\n",
      "    'torch_dtype': torch.float16,\n",
      "    'device_map': 'auto',\n",
      "    'quantization_config': BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": true,\n",
      "  \"_load_in_8bit\": false,\n",
      "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": true,\n",
      "  \"load_in_8bit\": false,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      ",\n",
      "    'rope_scaling': {'type': 'linear', 'factor': 4.0}}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:13<00:00,  6.56s/it]\n",
      "\u001b[2;36m11:41:07-590564\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m LOADER: \u001b[32m\"Transformers\"\u001b[0m                                 \n",
      "\u001b[2;36m11:41:07-591757\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRUNCATION LENGTH: \u001b[1;36m16384\u001b[0m                               \n",
      "\u001b[2;36m11:41:07-592588\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m INSTRUCTION TEMPLATE: \u001b[32m\"Custom \u001b[0m\u001b[32m(\u001b[0m\u001b[32mobtained from model \u001b[0m    \n",
      "\u001b[2;36m                \u001b[0m         \u001b[32mmetadata\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m                                             \n",
      "\u001b[2;36m11:41:07-593549\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Loaded the model in \u001b[1;36m15.14\u001b[0m seconds.                     \n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Running on public URL: https://194665e7e329f4b25d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
      "Output generated in 5.07 seconds (15.57 tokens/s, 79 tokens, context 170, seed 143381173)\n",
      "Output generated in 7.83 seconds (15.59 tokens/s, 122 tokens, context 265, seed 1359483876)\n",
      "Output generated in 10.09 seconds (16.55 tokens/s, 167 tokens, context 399, seed 2055126110)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "if Path.cwd().name != 'text-generation-webui':\n",
    "  print(\"Installing the webui...\")\n",
    "\n",
    "  !git clone https://github.com/oobabooga/text-generation-webui\n",
    "  %cd text-generation-webui\n",
    "\n",
    "  torver = torch.__version__\n",
    "  print(f\"TORCH: {torver}\")\n",
    "  is_cuda118 = '+cu118' in torver  # 2.1.0+cu118\n",
    "\n",
    "  if is_cuda118:\n",
    "    !python -m pip install --upgrade torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "  else:\n",
    "    !python -m pip install --upgrade torch==2.2.1 torchvision==0.17.1 torchaudio==2.2.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "  textgen_requirements = open('requirements.txt').read().splitlines()\n",
    "  if is_cuda118:\n",
    "      textgen_requirements = [req.replace('+cu121', '+cu118').replace('+cu122', '+cu118') for req in textgen_requirements]\n",
    "  with open('temp_requirements.txt', 'w') as file:\n",
    "      file.write('\\n'.join(textgen_requirements))\n",
    "\n",
    "  !pip install -r temp_requirements.txt --upgrade\n",
    "\n",
    "  print(\"\\033[1;32;1m\\n --> If you see a warning about \\\"previously imported packages\\\", just ignore it.\\033[0;37;0m\")\n",
    "  print(\"\\033[1;32;1m\\n --> There is no need to restart the runtime.\\n\\033[0;37;0m\")\n",
    "\n",
    "  try:\n",
    "    import flash_attn\n",
    "  except:\n",
    "    !pip uninstall -y flash_attn\n",
    "\n",
    "# Parameters\n",
    "model_url = \"https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct\" #Chosen model\n",
    "branch = \"main\" #@param {type:\"string\"}\n",
    "command_line_flags = \"--n-gpu-layers 128 --load-in-4bit --use_double_quant\" #Command line flags\n",
    "api = False #@param {type:\"boolean\"}\n",
    "\n",
    "if api:\n",
    "  for param in ['--api', '--public-api']:\n",
    "    if param not in command_line_flags:\n",
    "      command_line_flags += f\" {param}\"\n",
    "\n",
    "model_url = model_url.strip()\n",
    "if model_url != \"\":\n",
    "    if not model_url.startswith('http'):\n",
    "        model_url = 'https://huggingface.co/' + model_url\n",
    "\n",
    "    # Download the model\n",
    "    url_parts = model_url.strip('/').strip().split('/')\n",
    "    output_folder = f\"{url_parts[-2]}_{url_parts[-1]}\"\n",
    "    branch = branch.strip('\"\\' ')\n",
    "    if branch.strip() not in ['', 'main']:\n",
    "        output_folder += f\"_{branch}\"\n",
    "        !python download-model.py {model_url} --branch {branch}\n",
    "    else:\n",
    "        !python download-model.py {model_url}\n",
    "else:\n",
    "    output_folder = \"\"\n",
    "\n",
    "# Start the web UI\n",
    "cmd = f\"python server.py --share\"\n",
    "if output_folder != \"\":\n",
    "    cmd += f\" --model {output_folder}\"\n",
    "cmd += f\" {command_line_flags}\"\n",
    "print(cmd)\n",
    "!$cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
